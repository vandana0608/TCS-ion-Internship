{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mainprgforemodetectFV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfhIO3otLYRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8701dd77-5890-460b-8f9f-ed0c7e42129a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZVrId_4Kgcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#main function\n",
        "from nltk import *\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    with open('joyToken.txt', 'r') as myfile:\n",
        "        joy = myfile.read()\n",
        "    joySentences = sent_tokenize(joy)\n",
        "    \n",
        "    with open('saddnessToken.txt', 'r') as myfile:\n",
        "        saddness = myfile.read()\n",
        "    saddnessSentences = sent_tokenize(saddness)\n",
        "    \n",
        "    with open('shameToken.txt', 'r') as myfile:\n",
        "        shame = myfile.read()\n",
        "    shameSentences = sent_tokenize(shame)\n",
        "    \n",
        "    with open('lexicon_dictionary.txt', 'r') as myfile:\n",
        "        lexicon_dictionary = myfile.read()\n",
        "    lexicon_dictionary = lexicon_dictionary.split('\\n')\n",
        "    a = 0\n",
        "    for x in lexicon_dictionary :\n",
        "        lexicon_dictionary[a] = x.split(' ')\n",
        "        a = a + 1\n",
        "    \n",
        "    s = LancasterStemmer()\n",
        "    unwantedWordes = ['the' , 'a', 'is' , 'was' , 'are',\n",
        "                      'were' , 'to', 'at', 'i' , 'my',\n",
        "                      'on' , 'me'  , 'of' , '.' , 'in' ,\n",
        "                      'that' , 'he' , 'she' , 'it' , 'by']\n",
        "    \n",
        "    if not os.path.isfile('featureVectors.csv'):\n",
        "        open('featureVectors.csv' , 'w')   \n",
        "    with open('featureVectors.csv', 'w') as featuresFile:\n",
        "            featuresFile.write('')\n",
        "            \n",
        "    for i in range(0, a - 1): \n",
        "        lexicon_dictionary[i][0] = s.stem(lexicon_dictionary[i][0])\n",
        "    \n",
        "    # create feature Vector to each sentences in joy file\n",
        "    for x in joySentences:\n",
        "        featureVector = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        words = word_tokenize(x)\n",
        "        for y in words :\n",
        "            y = s.stem(y)\n",
        "            y = y.lower()\n",
        "            if y in unwantedWordes != -1:\n",
        "                continue\n",
        "            for i in range(0, a - 1): \n",
        "                if y == lexicon_dictionary[i][0]:\n",
        "                    for j in range(0, 10):\n",
        "                        featureVector[j] = featureVector[j] + int (lexicon_dictionary[i][j + 1])\n",
        "                    break\n",
        "        featureVector.append(0)\n",
        "        \n",
        "        # write this feature vector to featureVectors File\n",
        "        for k in range (0, 10):\n",
        "            with open('featureVectors.csv', 'a') as featuresFile:\n",
        "                featuresFile.write(str(featureVector[k]) + ',')\n",
        "        with open('featureVectors.csv', 'a') as featuresFile:\n",
        "            featuresFile.write(str(featureVector[10]) + '\\n')\n",
        "    \n",
        "    # create feature Vector to each sentences in sadness file\n",
        "    for x in saddnessSentences:\n",
        "        featureVector = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        words = word_tokenize(x)\n",
        "        for y in words :\n",
        "            y = s.stem(y)\n",
        "            y = y.lower()\n",
        "            if y in unwantedWordes != -1:\n",
        "                continue\n",
        "            for i in range(0, a - 1): \n",
        "                if y == lexicon_dictionary[i][0]:\n",
        "                    for j in range(0, 10):\n",
        "                        featureVector[j] = featureVector[j] + int (lexicon_dictionary[i][j + 1])\n",
        "                    break\n",
        "        featureVector.append(1)\n",
        "        \n",
        "        # write this feature vector to featureVectors File\n",
        "        for k in range (0, 10):\n",
        "            with open('featureVectors.csv', 'a') as featuresFile:\n",
        "                featuresFile.write(str(featureVector[k]) + ',')\n",
        "        with open('featureVectors.csv', 'a') as featuresFile:\n",
        "            featuresFile.write(str(featureVector[10]) + '\\n')\n",
        "    \n",
        "    \n",
        "    # create feature Vector to each sentences in shame file\n",
        "    \n",
        "    for x in shameSentences:\n",
        "        featureVector = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        words = word_tokenize(x)\n",
        "        for y in words :\n",
        "            y = s.stem(y)\n",
        "            y = y.lower()\n",
        "            if y in unwantedWordes != -1:\n",
        "                continue\n",
        "            for i in range(0, a - 1): \n",
        "                if y == lexicon_dictionary[i][0]:\n",
        "                    for j in range(0, 10):\n",
        "                        featureVector[j] = featureVector[j] + int (lexicon_dictionary[i][j + 1])\n",
        "                    break\n",
        "        featureVector.append(2)\n",
        "        \n",
        "        # write this feature vector to featureVectors File\n",
        "        for k in range (0, 10):\n",
        "            with open('featureVectors.csv', 'a') as featuresFile:\n",
        "                featuresFile.write(str(featureVector[k]) + ',')\n",
        "        with open('featureVectors.csv', 'a') as featuresFile:\n",
        "            featuresFile.write(str(featureVector[10]) + '\\n')\n",
        "    \n",
        "    with open('featureVectors.csv', 'r') as myfile:\n",
        "        features = myfile.read()\n",
        "    features = features.split('\\n')\n",
        "    with open('featureVectors.csv', 'w') as featuresFile:\n",
        "            featuresFile.write('')\n",
        "    \n",
        "    for i in range (0, 1040):\n",
        "        with open('featureVectors.csv', 'a') as featuresFile:\n",
        "            featuresFile.write(features[i] + '\\n')\n",
        "            featuresFile.write(features[i + 1040] + '\\n')\n",
        "            featuresFile.write(features[3117 - i] + '\\n')\n",
        "    \n",
        "    \n",
        "    pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLPM5-ZbLzap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "b8181d8d-18d9-45cd-a336-4216cbb36e1a"
      },
      "source": [
        "#Emotion of Sentences\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':  \n",
        "    # enter Your Sentences in Sentences.txt File\n",
        "    # Only code needed to  Load Code\n",
        "    json_file = open(\"model.json\", 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    model.load_weights(\"model.h5\")\n",
        "    print(\"Loaded model from disk\")\n",
        "    ###########################\n",
        "    with open('lexicon_dictionary.txt', 'r') as myfile:\n",
        "        lexicon_dictionary = myfile.read()\n",
        "    lexicon_dictionary = lexicon_dictionary.split('\\n')\n",
        "    a = 0\n",
        "\n",
        "    for x in lexicon_dictionary :\n",
        "        lexicon_dictionary[a] = x.split(' ')\n",
        "        a = a + 1\n",
        "    \n",
        "    with open('sentences.txt', 'r') as myfile:\n",
        "        sen = myfile.read()\n",
        "    sentences = sent_tokenize(sen)\n",
        "    \n",
        "    if not os.path.isfile('featureVectorForSentence.csv'):\n",
        "        open('featureVectorForSentence.csv' , 'w')   \n",
        "    with open('featureVectorForSentence.csv', 'w') as featuresFile:\n",
        "            featuresFile.write('')\n",
        "    \n",
        "    s = LancasterStemmer()\n",
        "    unwantedWordes = ['the' , 'a', 'is' , 'was' , 'are',\n",
        "                      'were' , 'to', 'at', 'i' , 'my',\n",
        "                      'on' , 'me'  , 'of' , '.' , 'in' ,\n",
        "                      'that' , 'he' , 'she' , 'it' , 'by']\n",
        "    for i in range(0, a - 1): \n",
        "        lexicon_dictionary[i][0] = s.stem(lexicon_dictionary[i][0])\n",
        "    \n",
        "    for x in sentences:\n",
        "        featureVector = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "        words = word_tokenize(x)\n",
        "        for y in words :\n",
        "            y = s.stem(y)\n",
        "            y = y.lower()\n",
        "            if y in unwantedWordes != -1:\n",
        "                continue\n",
        "            for i in range(0, a - 1): \n",
        "                if y == lexicon_dictionary[i][0]:\n",
        "                    for j in range(0, 10):\n",
        "                        featureVector[j] = featureVector[j] + int (lexicon_dictionary[i][j + 1])\n",
        "                    break\n",
        "        # write this feature vector to featureVectors File\n",
        "        for k in range (0, 9):\n",
        "            with open('featureVectorForSentence.csv', 'a') as featuresFile:\n",
        "                featuresFile.write(str(featureVector[k]) + ',')\n",
        "        with open('featureVectorForSentence.csv', 'a') as featuresFile:\n",
        "            featuresFile.write(str(featureVector[9]) + '\\n')\n",
        "    # to avoid one Sentence Error\n",
        "    for k in range (0, 9):\n",
        "        with open('featureVectorForSentence.csv', 'a') as featuresFile:\n",
        "                featuresFile.write(str(featureVector[k]) + ',')\n",
        "    with open('featureVectorForSentence.csv', 'a') as featuresFile:\n",
        "        featuresFile.write(str(featureVector[9]) + '\\n')\n",
        "        \n",
        "    \n",
        "    dataset = numpy.loadtxt(\"featureVectorForSentence.csv\", delimiter=\",\")\n",
        "    X = dataset[ :-1, :]\n",
        "    \n",
        "    predictions = model.predict(X)\n",
        "    rounded = numpy.around(predictions, decimals=0)\n",
        "    print (rounded)\n",
        "    \n",
        "    c = 1\n",
        "    shame = 0 \n",
        "    joy = 0 \n",
        "    sadness = 0\n",
        "    print(\"Emotions For Each Sentences in sentences.txt File\")\n",
        "    for x in rounded:\n",
        "        if x[0] == 1 and x[1] == 0 and x[2] == 0:\n",
        "            joy = joy + 1\n",
        "            print(\"Sentence Number \" + str(c) + \" is JOY\")\n",
        "        elif x[0] == 0 and x[1] == 1 and x[2] == 0:\n",
        "            sadness = sadness + 1\n",
        "            print(\"Sentence Number \" + str(c) + \" is Sadness\")\n",
        "        elif x[0] == 0 and x[1] == 0 and x[2] == 1:\n",
        "            shame = shame + 1\n",
        "            print(\"Sentence Number \" + str(c) + \" is Shame\")\n",
        "        c = c + 1\n",
        "    print (\"Joy :\" + str(joy))\n",
        "    print (\"sadness :\" + str(sadness))\n",
        "    print (\"shame :\" + str(shame))\n",
        "    \n",
        "    \n",
        "    pass\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "[[0. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n",
            "Emotions For Each Sentences in sentences.txt File\n",
            "Sentence Number 2 is JOY\n",
            "Sentence Number 3 is Shame\n",
            "Sentence Number 4 is Sadness\n",
            "Sentence Number 5 is Sadness\n",
            "Sentence Number 6 is Sadness\n",
            "Sentence Number 7 is Sadness\n",
            "Sentence Number 8 is Shame\n",
            "Sentence Number 9 is Shame\n",
            "Joy :1\n",
            "sadness :4\n",
            "shame :3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjY6hLhMMJgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50a1a9b7-87a4-4e21-b90b-ff01433fec57"
      },
      "source": [
        "#ML code for Feature Vectorization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy\n",
        "\n",
        "dataset = numpy.loadtxt(\"featureVectors.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "# Train Set\n",
        "X = dataset[:, 0:10]\n",
        "y_int = dataset[:, 10:]\n",
        "Y = to_categorical(y_int)\n",
        "\n",
        "# Test Set\n",
        "XTest = dataset[0:200, 0:10]\n",
        "ytest_int = dataset[0:200, 10:]\n",
        "yTest = to_categorical(ytest_int)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(output_dim=100, init='uniform', activation='relu'   , input_dim=10))\n",
        "model.add(Dense(output_dim=100 , init='uniform', activation='relu'   , input_dim=100))\n",
        "model.add(Dense(output_dim=3   , init='uniform', activation='softmax', input_dim=100))\n",
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, nb_epoch=25, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(XTest, yTest)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
        "\n",
        "# Train Set\n",
        "X = dataset[:2900, 0:10]\n",
        "y_int = dataset[:2900, 10:]\n",
        "Y = to_categorical(y_int)\n",
        "# Test Set\n",
        "XTest = dataset[2900:, 0:10]\n",
        "ytest_int = dataset[2900:, 10:]\n",
        "yTest = to_categorical(ytest_int)\n",
        "# Fit the model\n",
        "model.fit(X, Y, nb_epoch=25, batch_size=10)\n",
        "# evaluate the model\n",
        "scores = model.evaluate(XTest, yTest)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
        "\n",
        "\n",
        "# #Only code needed to save model\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model.h5\")\n",
        "##################################"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10, units=100, kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=100, units=100, kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", input_dim=100, units=3, kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "3120/3120 [==============================] - 0s 140us/step - loss: 1.0417 - accuracy: 0.4439\n",
            "Epoch 2/25\n",
            "3120/3120 [==============================] - 0s 120us/step - loss: 0.9938 - accuracy: 0.5244\n",
            "Epoch 3/25\n",
            "3120/3120 [==============================] - 0s 115us/step - loss: 0.9760 - accuracy: 0.5308\n",
            "Epoch 4/25\n",
            "3120/3120 [==============================] - 0s 117us/step - loss: 0.9704 - accuracy: 0.5362\n",
            "Epoch 5/25\n",
            "3120/3120 [==============================] - 0s 111us/step - loss: 0.9607 - accuracy: 0.5375\n",
            "Epoch 6/25\n",
            "3120/3120 [==============================] - 0s 110us/step - loss: 0.9537 - accuracy: 0.5436\n",
            "Epoch 7/25\n",
            "3120/3120 [==============================] - 0s 111us/step - loss: 0.9525 - accuracy: 0.5413\n",
            "Epoch 8/25\n",
            "3120/3120 [==============================] - 0s 113us/step - loss: 0.9460 - accuracy: 0.5436\n",
            "Epoch 9/25\n",
            "3120/3120 [==============================] - 0s 117us/step - loss: 0.9417 - accuracy: 0.5413\n",
            "Epoch 10/25\n",
            "3120/3120 [==============================] - 0s 114us/step - loss: 0.9392 - accuracy: 0.5481\n",
            "Epoch 11/25\n",
            "3120/3120 [==============================] - 0s 120us/step - loss: 0.9325 - accuracy: 0.5583\n",
            "Epoch 12/25\n",
            "3120/3120 [==============================] - 0s 118us/step - loss: 0.9286 - accuracy: 0.5484\n",
            "Epoch 13/25\n",
            "3120/3120 [==============================] - 0s 118us/step - loss: 0.9277 - accuracy: 0.5587\n",
            "Epoch 14/25\n",
            "3120/3120 [==============================] - 0s 117us/step - loss: 0.9247 - accuracy: 0.5567\n",
            "Epoch 15/25\n",
            "3120/3120 [==============================] - 0s 119us/step - loss: 0.9213 - accuracy: 0.5641\n",
            "Epoch 16/25\n",
            "3120/3120 [==============================] - 0s 115us/step - loss: 0.9193 - accuracy: 0.5644\n",
            "Epoch 17/25\n",
            "3120/3120 [==============================] - 0s 117us/step - loss: 0.9127 - accuracy: 0.5696\n",
            "Epoch 18/25\n",
            "3120/3120 [==============================] - 0s 113us/step - loss: 0.9148 - accuracy: 0.5670\n",
            "Epoch 19/25\n",
            "3120/3120 [==============================] - 0s 118us/step - loss: 0.9061 - accuracy: 0.5737\n",
            "Epoch 20/25\n",
            "3120/3120 [==============================] - 0s 117us/step - loss: 0.9063 - accuracy: 0.5692\n",
            "Epoch 21/25\n",
            "3120/3120 [==============================] - 0s 112us/step - loss: 0.9014 - accuracy: 0.5673\n",
            "Epoch 22/25\n",
            "3120/3120 [==============================] - 0s 122us/step - loss: 0.9014 - accuracy: 0.5696\n",
            "Epoch 23/25\n",
            "3120/3120 [==============================] - 0s 126us/step - loss: 0.8990 - accuracy: 0.5654\n",
            "Epoch 24/25\n",
            "3120/3120 [==============================] - 0s 137us/step - loss: 0.8911 - accuracy: 0.5798\n",
            "Epoch 25/25\n",
            "3120/3120 [==============================] - 0s 134us/step - loss: 0.8889 - accuracy: 0.5689\n",
            "200/200 [==============================] - 0s 116us/step\n",
            "\n",
            "accuracy: 58.00%\n",
            "Epoch 1/25\n",
            "1410/2900 [=============>................] - ETA: 0s - loss: 0.8837 - accuracy: 0.5851"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2900/2900 [==============================] - 0s 117us/step - loss: 0.8841 - accuracy: 0.5741\n",
            "Epoch 2/25\n",
            "2900/2900 [==============================] - 0s 126us/step - loss: 0.8806 - accuracy: 0.5876\n",
            "Epoch 3/25\n",
            "2900/2900 [==============================] - 0s 120us/step - loss: 0.8759 - accuracy: 0.5841\n",
            "Epoch 4/25\n",
            "2900/2900 [==============================] - 0s 117us/step - loss: 0.8767 - accuracy: 0.5845\n",
            "Epoch 5/25\n",
            "2900/2900 [==============================] - 0s 118us/step - loss: 0.8678 - accuracy: 0.5859\n",
            "Epoch 6/25\n",
            "2900/2900 [==============================] - 0s 119us/step - loss: 0.8685 - accuracy: 0.5852\n",
            "Epoch 7/25\n",
            "2900/2900 [==============================] - 0s 121us/step - loss: 0.8607 - accuracy: 0.5945\n",
            "Epoch 8/25\n",
            "2900/2900 [==============================] - 0s 122us/step - loss: 0.8560 - accuracy: 0.5910\n",
            "Epoch 9/25\n",
            "2900/2900 [==============================] - 0s 123us/step - loss: 0.8540 - accuracy: 0.6024\n",
            "Epoch 10/25\n",
            "2900/2900 [==============================] - 0s 117us/step - loss: 0.8498 - accuracy: 0.6007\n",
            "Epoch 11/25\n",
            "2900/2900 [==============================] - 0s 122us/step - loss: 0.8460 - accuracy: 0.6062\n",
            "Epoch 12/25\n",
            "2900/2900 [==============================] - 0s 118us/step - loss: 0.8390 - accuracy: 0.6059\n",
            "Epoch 13/25\n",
            "2900/2900 [==============================] - 0s 117us/step - loss: 0.8383 - accuracy: 0.6062\n",
            "Epoch 14/25\n",
            "2900/2900 [==============================] - 0s 122us/step - loss: 0.8312 - accuracy: 0.6141\n",
            "Epoch 15/25\n",
            "2900/2900 [==============================] - 0s 117us/step - loss: 0.8288 - accuracy: 0.6197\n",
            "Epoch 16/25\n",
            "2900/2900 [==============================] - 0s 118us/step - loss: 0.8214 - accuracy: 0.6055\n",
            "Epoch 17/25\n",
            "2900/2900 [==============================] - 0s 129us/step - loss: 0.8165 - accuracy: 0.6197\n",
            "Epoch 18/25\n",
            "2900/2900 [==============================] - 0s 118us/step - loss: 0.8145 - accuracy: 0.6214\n",
            "Epoch 19/25\n",
            "2900/2900 [==============================] - 0s 125us/step - loss: 0.8081 - accuracy: 0.6200\n",
            "Epoch 20/25\n",
            "2900/2900 [==============================] - 0s 118us/step - loss: 0.8047 - accuracy: 0.6245\n",
            "Epoch 21/25\n",
            "2900/2900 [==============================] - 0s 116us/step - loss: 0.8013 - accuracy: 0.6272\n",
            "Epoch 22/25\n",
            "2900/2900 [==============================] - 0s 118us/step - loss: 0.7992 - accuracy: 0.6221\n",
            "Epoch 23/25\n",
            "2900/2900 [==============================] - 0s 120us/step - loss: 0.7911 - accuracy: 0.6259\n",
            "Epoch 24/25\n",
            "2900/2900 [==============================] - 0s 136us/step - loss: 0.7863 - accuracy: 0.6303\n",
            "Epoch 25/25\n",
            "2900/2900 [==============================] - 0s 139us/step - loss: 0.7837 - accuracy: 0.6334\n",
            "220/220 [==============================] - 0s 47us/step\n",
            "\n",
            "accuracy: 46.82%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}